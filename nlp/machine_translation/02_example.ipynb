{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Neural Machine Translation for French to English</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3aee62b9-47ce-e416-5816-8df7126fe690"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gzip\n",
    "import codecs as cds\n",
    "import re\n",
    "import time\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.contrib.seq2seq import TrainingHelper, GreedyEmbeddingHelper, BasicDecoder, dynamic_decode\n",
    "from tensorflow.contrib.seq2seq import BahdanauAttention, AttentionWrapper, sequence_loss\n",
    "from tensorflow.contrib.rnn import GRUCell, DropoutWrapper\n",
    "TOKEN_GO = '<GO>'\n",
    "TOKEN_EOS = '<EOS>'\n",
    "TOKEN_PAD = '<PAD>'\n",
    "TOKEN_UNK = '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "654175ff-07c7-455d-1b8f-d702cee211c4"
   },
   "outputs": [],
   "source": [
    "frdata=[]\n",
    "endata=[]\n",
    "with open('data/train_fr_lines.txt') as frfile:\n",
    "    for li in frfile:\n",
    "        frdata.append(li)\n",
    "with open('data/train_en_lines.txt') as enfile:\n",
    "    for li in enfile:\n",
    "        endata.append(li)\n",
    "mtdata = pd.DataFrame({'FR':frdata,'EN':endata})\n",
    "mtdata['FR_len'] = mtdata['FR'].apply(lambda x: len(x.split(' ')))\n",
    "mtdata['EN_len'] = mtdata['EN'].apply(lambda x: len(x.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mtdata['FR'].head(2).values)\n",
    "print(mtdata['EN'].head(2).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "48f10124-4c1b-1f09-512d-352c068de1b4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mtdata_fr = []\n",
    "for fr in mtdata.FR:\n",
    "    mtdata_fr.append(fr)\n",
    "mtdata_en = []\n",
    "for en in mtdata.EN:\n",
    "    mtdata_en.append(en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3f980247-3c32-240d-7d3d-7b0c3c6c13e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words(words_dict, text):\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in words_dict:\n",
    "                words_dict[word] = 1\n",
    "            else:\n",
    "                words_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3e9ce130-88f4-8779-5b5f-86f2a23ab347"
   },
   "outputs": [],
   "source": [
    "word_counts_dict_fr = {}\n",
    "word_counts_dict_en = {}\n",
    "count_words(word_counts_dict_fr, mtdata_fr)\n",
    "count_words(word_counts_dict_en, mtdata_en)\n",
    "            \n",
    "print(\"Total French words in Vocabulary:\", len(word_counts_dict_fr))\n",
    "print(\"Total English words in Vocabulary\", len(word_counts_dict_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_wd_vector_matrix(vect_f):\n",
    "    emb_index = {}\n",
    "    with cds.open(vect_f, 'r', 'utf-8') as fl:\n",
    "        for i, wd_li in enumerate(fl):\n",
    "            sr = wd_li.split()\n",
    "            if(len(sr)<26):\n",
    "                continue\n",
    "            wd = sr[0]\n",
    "            emb = np.asarray(sr[1:], dtype='float32')\n",
    "            emb_index[wd] = emb\n",
    "    return emb_index\n",
    "embs_index = build_wd_vector_matrix('../../temp/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0be42a13-70b2-e9cc-7468-1247b01f109c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_word2id_mapping(word_counts_dict):\n",
    "    word2int = {} \n",
    "    count_threshold = 20\n",
    "    value = 0\n",
    "    for word, count in word_counts_dict.items():\n",
    "        if count >= count_threshold or word in embs_index:\n",
    "            word2int[word] = value\n",
    "            value += 1\n",
    "\n",
    "\n",
    "    special_codes = [TOKEN_UNK,TOKEN_PAD,TOKEN_EOS,TOKEN_GO]   \n",
    "\n",
    "    for code in special_codes:\n",
    "        word2int[code] = len(word2int)\n",
    "\n",
    "    int2word = {}\n",
    "    for word, value in word2int.items():\n",
    "        int2word[value] = word\n",
    "    return word2int,int2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_embeddings(word2int):\n",
    "    embedding_dim = 50\n",
    "    nwords = len(word2int)\n",
    "\n",
    "    word_emb_matrix = np.zeros((nwords, embedding_dim), dtype=np.float32)\n",
    "    for word, i in word2int.items():\n",
    "        if word in embs_index:\n",
    "            word_emb_matrix[i] = embs_index[word]\n",
    "        else:\n",
    "            new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "            word_emb_matrix[i] = new_embedding\n",
    "    return word_emb_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4401990d-4baf-3f30-becc-3a6149716b56"
   },
   "outputs": [],
   "source": [
    "fr_word2int,fr_int2word = build_word2id_mapping(word_counts_dict_fr)\n",
    "en_word2int,en_int2word = build_word2id_mapping(word_counts_dict_en)\n",
    "fr_embs_mat = build_embeddings(fr_word2int)\n",
    "en_embs_mat = build_embeddings(en_word2int)\n",
    "print(\"Length of french word embeddings: \", len(fr_embs_mat))\n",
    "print(\"Length of english word embeddings: \", len(en_embs_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "25cfd0e3-ae3d-8728-1c82-1a61bb06aa0e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_sentence_to_ids(text, word2int, eos=False):\n",
    "    wordints = []\n",
    "    word_count = 0\n",
    "    for sentence in text:\n",
    "        sentence2ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in word2int:\n",
    "                sentence2ints.append(word2int[word])\n",
    "            else:\n",
    "                sentence2ints.append(word2int[TOKEN_UNK])\n",
    "        if eos:\n",
    "            sentence2ints.append(word2int[TOKEN_EOS])\n",
    "        wordints.append(sentence2ints)\n",
    "    return wordints, word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "360cfdf4-ad4c-0316-56d3-70b6206e75e4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_fr, word_count_fr = convert_sentence_to_ids(mtdata_fr, fr_word2int)\n",
    "id_en, word_count_en = convert_sentence_to_ids(mtdata_en, en_word2int, eos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2a0ae7cd-a845-23dc-3563-ad133e2f02b4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unknown_tokens(sentence, word2int):\n",
    "    unk_token_count = 0\n",
    "    for word in sentence:\n",
    "        if word == word2int[TOKEN_UNK]:\n",
    "            unk_token_count += 1\n",
    "    return unk_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "50d631a2-fb5a-bb0d-6155-9cd15e70835b"
   },
   "outputs": [],
   "source": [
    "en_filtered = []\n",
    "fr_filtered = []\n",
    "max_en_length = int(mtdata.EN_len.max())\n",
    "max_fr_length = int(mtdata.FR_len.max())\n",
    "min_length = 4\n",
    "unknown_token_en_limit = 10\n",
    "unknown_token_fr_limit = 10\n",
    "\n",
    "for count,text in enumerate(id_en):\n",
    "    unknown_token_en = unknown_tokens(id_en[count],en_word2int)\n",
    "    unknown_token_fr = unknown_tokens(id_fr[count],fr_word2int)\n",
    "    en_len = len(id_en[count])\n",
    "    fr_len = len(id_fr[count])\n",
    "    if( (unknown_token_en>unknown_token_en_limit) or (unknown_token_fr>unknown_token_fr_limit) or \n",
    "       (en_len<min_length) or (fr_len<min_length) ):\n",
    "        continue\n",
    "    fr_filtered.append(id_fr[count])\n",
    "    en_filtered.append(id_en[count])\n",
    "print(\"Length of filtered french/english sentences: \", len(fr_filtered), len(en_filtered) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "34d28c5f-8016-6b36-664e-3d5ee3db745d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    dat_inps = tf.placeholder(tf.int32, [None, None], name='dat_inps')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    rt_lr = tf.placeholder(tf.float32, name='rt_lr')\n",
    "    drp_rt = tf.placeholder(tf.float32, name='drp_rt')\n",
    "    en_len = tf.placeholder(tf.int32, (None,), name='en_len')\n",
    "    max_en_len = tf.reduce_max(en_len, name='max_en_len')\n",
    "    fr_len = tf.placeholder(tf.int32, (None,), name='fr_len')\n",
    "    return dat_inps, targets, rt_lr, drp_rt, en_len, max_en_len, fr_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9c9b6087-3c28-478d-d311-4213e1c59654",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, word2int, batch_size):\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    decoding_input = tf.concat([tf.fill([batch_size, 1], word2int[TOKEN_GO]), ending], 1)\n",
    "    return decoding_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d675562b-a9e0-df71-6979-a052fb78dcbc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_recnet_cell(recnet_cell_sz,drp_rt):\n",
    "    c_recnet= GRUCell(recnet_cell_sz)\n",
    "    c_recnet= DropoutWrapper(c_recnet, input_keep_prob = drp_rt)\n",
    "    return c_recnet\n",
    "\n",
    "def encoding_layer(recnet_cell_sz, len_seq, n_layers, recnet_inp, drp_rt):\n",
    "    for l in range(n_layers):\n",
    "        with tf.variable_scope('encoding_l_{}'.format(l)):\n",
    "            fw_recnet = get_recnet_cell(recnet_cell_sz,drp_rt)\n",
    "            bw_recnet = get_recnet_cell(recnet_cell_sz,drp_rt)\n",
    "            op_enc, st_enc = tf.nn.bidirectional_dynamic_rnn(fw_recnet, bw_recnet, \n",
    "                                                                    recnet_inp,\n",
    "                                                                    len_seq,\n",
    "                                                                    dtype=tf.float32)\n",
    "    op_enc = tf.concat(op_enc,2)\n",
    "    return op_enc, st_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "524d0246-ddae-b485-5ea4-11ad476447f4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_decoding_layer(inp_dec_emb, en_len, c_dec, st_init, op_layer, \n",
    "                            v_sz, max_en_len):\n",
    "    helper = TrainingHelper(inputs=inp_dec_emb,sequence_length=en_len, time_major=False)\n",
    "    dec = BasicDecoder(c_dec,helper,st_init,op_layer) \n",
    "    lgits, _, _ = dynamic_decode(dec,output_time_major=False,impute_finished=True, \n",
    "                                  maximum_iterations=max_en_len)\n",
    "    return lgits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6044b206-7f27-5304-4896-06d388af0949",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference_decoding_layer(embs, tk_st, toks_en, c_dec, initial_state, lyr_op,\n",
    "                             max_en_len, bt_sz):\n",
    "    \n",
    "    toks_st = tf.tile(tf.constant([tk_st], dtype=tf.int32), [bt_sz], name='toks_st')\n",
    "    inf_helper = GreedyEmbeddingHelper(embs,toks_st,toks_en)\n",
    "    dec_inf = BasicDecoder(c_dec,inf_helper,initial_state,lyr_op)       \n",
    "    inf_lgits, _, _ = dynamic_decode(dec_inf,output_time_major=False,impute_finished=True,\n",
    "                                                            maximum_iterations=max_en_len)\n",
    "    return inf_lgits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4b50746f-8f78-0253-9178-56c62e4ac1bf",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer(dec_emb_inp, embs, enc_op, st_enc, v_size, fr_len, \n",
    "                   en_len,max_en_len, recnet_sz, word2int, drp_rt, bt_sz, lyr_n):\n",
    "    \n",
    "    for l in range(lyr_n):\n",
    "        with tf.variable_scope('dec_rnn_layer_{}'.format(l)):\n",
    "            gru = tf.contrib.rnn.GRUCell(rnn_len)\n",
    "            c_dec = tf.contrib.rnn.DropoutWrapper(gru,input_keep_prob = drp_rt)\n",
    "    out_l = Dense(v_size, kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    attention = BahdanauAttention(recnet_sz, enc_op,fr_len,\n",
    "                                                  normalize=False,\n",
    "                                                  name='BahdanauAttention')\n",
    "    c_dec =  AttentionWrapper(c_dec,attention,rnn_len)\n",
    "    st_attn_zero = c_dec.zero_state(bt_sz , tf.float32 )\n",
    "    st_attn_zero = st_attn_zero.clone(cell_state = st_enc[0])\n",
    "    with tf.variable_scope(\"decoding_layer\"):\n",
    "        lgits_tr = training_decoding_layer(dec_emb_inp, \n",
    "                                                  en_len, \n",
    "                                                  c_dec, \n",
    "                                                  st_attn_zero,\n",
    "                                                  out_l,\n",
    "                                                  v_size, \n",
    "                                                  max_en_len)\n",
    "    with tf.variable_scope(\"decoding_layer\", reuse=True):\n",
    "        lgits_inf = inference_decoding_layer(embs,  word2int[TOKEN_GO], \n",
    "                                                    word2int[TOKEN_EOS],\n",
    "                                                    c_dec, \n",
    "                                                    st_attn_zero, \n",
    "                                                    out_l,\n",
    "                                                    max_en_len,\n",
    "                                                    bt_sz)\n",
    "\n",
    "    return lgits_tr, lgits_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "19ddcf22-4f6a-d531-071a-021b42b643e3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_seqtoseq(dat_inp, target_en_data, drp_rt, fr_len, en_len, max_en_len, \n",
    "                  v_size, recnet_sz, lyr_n, word2int_en, bt_sz):\n",
    "    \n",
    "    inp_wd_embs = tf.Variable(fr_embs_mat, name=\"inp_wd_embs\")\n",
    "    enc_emb_inp = tf.nn.embedding_lookup(inp_wd_embs, dat_inp)\n",
    "    op_enc, st_enc = encoding_layer(recnet_sz, fr_len, lyr_n, enc_emb_inp, drp_rt)\n",
    "    \n",
    "    dec_inp = process_encoding_input(target_en_data, word2int_en, bt_sz)\n",
    "    dec_emb_inp = tf.nn.embedding_lookup(en_embs_mat, dec_inp)\n",
    "    \n",
    "    tr_lgits, inf_lgits  = decoding_layer(dec_emb_inp, en_embs_mat,op_enc,st_enc, v_size, fr_len, \n",
    "                                                        en_len, max_en_len,\n",
    "                                                        recnet_sz, word2int_en, \n",
    "                                                        drp_rt, bt_sz,lyr_n)\n",
    "    return tr_lgits, inf_lgits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "725e92bf-2309-1a78-c771-641a42b440c6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sents_pad(sentences_batch,word2int):\n",
    "    max_sentence = max([len(sentence) for sentence in sentences_batch])\n",
    "    return [sentence + [word2int[TOKEN_PAD]] * (max_sentence - len(sentence)) for sentence in sentences_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "47e4f70a-6377-68dd-c06c-eed674b2bb3f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(en_text, fr_text, bt_sz):\n",
    "    for bt_idx in range(0, len(fr_text)//bt_sz):\n",
    "        start_idx = bt_idx * bt_sz\n",
    "        en_bt = en_text[start_idx:start_idx + bt_sz]\n",
    "        fr_bt = fr_text[start_idx:start_idx + bt_sz]\n",
    "        pad_en_bt = np.array(sents_pad(en_bt, en_word2int))\n",
    "        pad_fr_bt = np.array(sents_pad(fr_bt,fr_word2int))\n",
    "\n",
    "        pad_en_lens = []\n",
    "        for en_b in pad_en_bt:\n",
    "            pad_en_lens.append(len(en_b))\n",
    "        \n",
    "        pad_fr_lens = []\n",
    "        for fr_b in pad_fr_bt:\n",
    "            pad_fr_lens.append(len(fr_b))\n",
    "        \n",
    "        yield pad_en_bt, pad_fr_bt, pad_en_lens, pad_fr_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "77299c4b-a3cf-785b-981a-42a1bb3a2033",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "bt_sz = 64\n",
    "rnn_len = 256\n",
    "n_layers = 2\n",
    "lr = 0.005\n",
    "dr_prob = 0.75\n",
    "logs_path='/tmp/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "68781626-8bf4-0a23-4bb2-f24a5762fa1e"
   },
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    dat_inp, targets, lr_rt, drp_rt, en_len, max_en_len, fr_len = model_inputs()\n",
    "\n",
    "    lgits_tr, lgits_inf = model_seqtoseq(tf.reverse(dat_inp, [-1]),\n",
    "                                                      targets, \n",
    "                                                      drp_rt,   \n",
    "                                                      fr_len,\n",
    "                                                      en_len,\n",
    "                                                      max_en_len,\n",
    "                                                      len(en_word2int)+1,\n",
    "                                                      rnn_len, \n",
    "                                                      n_layers, \n",
    "                                                      en_word2int,\n",
    "                                                      bt_sz)\n",
    "    \n",
    "    lgits_tr = tf.identity(lgits_tr.rnn_output, 'lgits_tr')\n",
    "    lgits_inf = tf.identity(lgits_inf.sample_id, name='predictions')\n",
    "    \n",
    "    seq_masks = tf.sequence_mask(en_len, max_en_len, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimizer\"):\n",
    "        tr_cost = sequence_loss(lgits_tr,targets,seq_masks)\n",
    "        optimizer = tf.train.AdamOptimizer(lr_rt)\n",
    "        grds = optimizer.compute_gradients(tr_cost)\n",
    "        cap_grds = [(tf.clip_by_value(gr, -5., 5.), var) for gr, var in grds \n",
    "                        if gr is not None]\n",
    "        train_op = optimizer.apply_gradients(cap_grds)\n",
    "    tf.summary.scalar(\"cost\", tr_cost)\n",
    "print(\"Graph created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6368ba0d-4083-e182-ca38-5356b307e09f"
   },
   "outputs": [],
   "source": [
    "min_learning_rate = 0.0006\n",
    "display_step = 20 \n",
    "stop_early_count = 0 \n",
    "stop_early_max_count = 3 \n",
    "per_epoch = 10 \n",
    "\n",
    "\n",
    "update_loss = 0 \n",
    "bt_loss = 0\n",
    "s_upd_loss = [] \n",
    "\n",
    "en_tr = en_filtered[0:30000]\n",
    "fr_tr = fr_filtered[0:30000]\n",
    "update_check = (len(fr_tr)//bt_sz//per_epoch)-1\n",
    "checkpoint = logs_path + 'best_so_far_model.ckpt' \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    s_writer = tf.summary.FileWriter(logs_path, graph=train_graph)\n",
    "    op_summ_merged = tf.summary.merge_all()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        bt_loss = 0\n",
    "        for bt_i, (en_batch, fr_batch, en_text_len, fr_text_len) in enumerate(\n",
    "                get_batches(en_tr, fr_tr, bt_sz)):\n",
    "            before = time.time()\n",
    "            _,loss,res_summ = sess.run(\n",
    "                [train_op, tr_cost,op_summ_merged],\n",
    "                {dat_inp: fr_batch,\n",
    "                 targets: en_batch,\n",
    "                 lr_rt: lr,\n",
    "                 en_len: en_text_len,\n",
    "                 fr_len: fr_text_len,\n",
    "                 drp_rt: dr_prob})\n",
    "            bt_loss += loss\n",
    "            update_loss += loss\n",
    "            after = time.time()\n",
    "            batch_time = after - before\n",
    "            s_writer.add_summary(res_summ, epoch_i * bt_sz + bt_i)\n",
    "            if bt_i % display_step == 0 and bt_i > 0:\n",
    "                print('** Epoch {:>3}/{} Batch {:>4}/{} - Batch Loss: {:>6.3f}, seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              bt_i, \n",
    "                              len(fr_filtered) // bt_sz, \n",
    "                              bt_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                bt_loss = 0\n",
    "\n",
    "            if bt_i % update_check == 0 and bt_i > 0:\n",
    "                print(\"Average loss:\", round(update_loss/update_check,3))\n",
    "                s_upd_loss.append(update_loss)\n",
    "                \n",
    "                if update_loss <= min(s_upd_loss):\n",
    "                    print('Saving model') \n",
    "                    stop_early_count = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early_count += 1\n",
    "                    if stop_early_count == stop_early_max_count:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "\n",
    "        if stop_early_count == stop_early_max_count:\n",
    "            print(\"Stopping Training.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "265fd2f2-cd5f-590d-1fa3-cf6eeedf89fe"
   },
   "outputs": [],
   "source": [
    "#random = np.random.randint(3000,len(fr_filtered))\n",
    "random = np.random.randint(0,3000)\n",
    "fr_text = fr_filtered[random]\n",
    "\n",
    "checkpoint = logs_path + 'best_so_far_model.ckpt'\n",
    "\n",
    "g_load = tf.Graph()\n",
    "with tf.Session(graph=g_load) as sess:\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "    dat_inp = g_load.get_tensor_by_name('dat_inps:0')\n",
    "    lgits = g_load.get_tensor_by_name('predictions:0')\n",
    "    fr_length = g_load.get_tensor_by_name('fr_len:0')\n",
    "    en_length = g_load.get_tensor_by_name('en_len:0')\n",
    "    dropout_prob = g_load.get_tensor_by_name('drp_rt:0')\n",
    "    result_lgits = sess.run(lgits, {dat_inp: [fr_text]*bt_sz, \n",
    "                                      en_length: [len(fr_text)], \n",
    "                                      fr_length: [len(fr_text)]*bt_sz,\n",
    "                                      dropout_prob: 1.0})[0] \n",
    "\n",
    "pad = en_word2int[TOKEN_PAD] \n",
    "\n",
    "#print('\\nOriginal Text:', input_sentence)\n",
    "\n",
    "print('\\nFrench Text')\n",
    "print('  Word Ids:    {}'.format([i for i in fr_text]))\n",
    "print('  Input Words: {}'.format(\" \".join( [fr_int2word[i] for i in fr_text ] )))\n",
    "\n",
    "print('\\nEnglish Text')\n",
    "print('  Word Ids:       {}'.format([i for i in result_lgits if i != pad]))\n",
    "print('  Response Words: {}'.format(\" \".join( [en_int2word[i]for i in result_lgits if i!=pad] )))\n",
    "print(' Ground Truth: {}'.format(\" \".join( [en_int2word[i] for i in en_filtered[random]] )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
